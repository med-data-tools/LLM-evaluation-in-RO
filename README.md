# LLM-evaluation-in-RO

## What is this?

This is the github repository hosting the code and a data file related to the following publication.

---
Authors:
Fabio Dennst√§dt, Max Schmerder, Elean Riggenbach, Lucas Mose, Katarina Bryjova, Nicolas Bachmann, Paul-Henry Mackeprang, Maiwand Ahmadsei, Dubravko Sinovcic, Paul Windisch, Daniel R Zwahlen, Susanne J Rogers, Oliver Riesterer, Martin Maffei, Eleni Gkika, Hathal Haddad, Jan C Peeken, Paul Martin Putora, Markus Glatzer, Florian Putz, Daniel Hoefler, Sebastian M Christ, Irina Filchenko, Janna Hastings, Roberto Gaio, Lawrence Chiang, Daniel Aebersold, Nikola Cihoric

Title:
Comparative Evaluation of a Medical Large Language Model in Answering Real-World Radiation Oncology Questions: A Multicenter Observational Study
DOI: 10.2196/69752

---

The publication is about a multicentric evaluation study investigating the performance of the medical open-source LLM Llama3-OpenBioLLM-70B in answering questions from real-life clinical practice of radiation therapy. The performance of the LLM was compared to that of clinical experts in a blinded review.


## Content

### script.ipynb

A python jupyter script used to run the LLM as reported in the article.

### questions used in the study with answers of LLM and evaluations.docx and .csv

A data set containing the 50 questions collected from real-life clinical practice of radiation therapy used in the study together with the answers of the LLM, the difficulty of the questions and the quality of the answers as evaluated in the study.
